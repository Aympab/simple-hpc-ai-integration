# Toy HPC simulation calling a Neural Network
Run `$ source env` to load environment variable and aliases needed (explained in
this README) 

## Create a python virtual env
- Creation : `python3 -m venv my-venv`
- Activation : `source my-env/bin/activate`
- Package installation : `python3 -m pip install ipykernel`

## Build with `make`

# ML Integration test

## ONNX Library
Append the `LD_LIBRARY_PATH` variable with the lib of onnxruntine :

```
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/source/simple-hpc/external/onnxruntime/lib/
```

### Converting a TF model to ONNX with **tf2onnx**
See the [tf2onnx](https://github.com/onnx/tensorflow-onnx) GitHub page to
convert a `saved_mode.pb` from tensorflow to ONNX format.

```
$ python -m tf2onnx.convert --saved-model tf/model/path --output saved_model.onnx
```

## TVM
Install the library and set everything up with [TVM's user tutorial](https://tvm.apache.org/docs/tutorial/index.html).

Export TVM_HOME
``` 
$ export TVM_HOME=/net/jabba/home0/am611608/source/tvm
$ export PYTHONPATH=$TVM_HOME/python:${PYTHONPATH}
```

To use `TVMC` with python, use the full command :
```
$ python -m tvm.driver.tvmc <subcommand> <options>
```

From [TVM official tutorial](https://tvm.apache.org/docs/tutorial/tvmc_command_line_driver.html#using-tvmc).

### [Compiling with TVM](https://tvm.apache.org/docs/tutorial/tvmc_command_line_driver.html#compiling-an-onnx-model-to-the-tvm-runtime) :

```
$ python -m tvm.driver.tvmc compile    \
         --target "llvm"               \
         --output toy-model-tvm.tar \
         toy-model.onnx
```

### Remarks
The model has to be correctly outputed in ONNX format. See
[pytorch's onnx export help page](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)
for more details. Here, the toymodel has been exported as :

```python
toy.eval() # To tell pytorch the model is going to be used for inference 

x = torch.randn(1, 8)
out = toy(x)

torch.onnx.export(
    toy,                #Model
    x,                  #Input
    "toy-model.onnx",   #Name of the generated file
    export_params=True, #Export bias and weight values
    opset_version=10,   #ONNX version
)
```
So you **need** to put an input example, and a output generated by the model.

To check if the ONNX file is correct, run :
```python
onnx_model = onnx.load("toy-model.onnx")
onnx.checker.check_model(onnx_model)
```